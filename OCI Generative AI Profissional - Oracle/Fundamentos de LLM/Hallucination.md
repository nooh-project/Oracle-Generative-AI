Hallucination é o nome dado ao texto gerado por LLMs que não são factuais ou não suportados pelos dados usados no treinamento do modelo. Não existe uma maneira de eliminar definitivamente esse problema, mas apenas reduzir sua ocorrência.

## Groundedness and Attributability

Basicamente é uma forma que a comunidade têm pesquisado de evitar as “hallucinations”, fazendo com que o modelo aprenda a citar e se basear nas fontes do texto que ele gerou.